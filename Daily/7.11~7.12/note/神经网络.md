# BP神经网络

+ 阈值可以理解为偏置，因为通过矩阵运算可得aw+b，b就是偏置所带来的量![](https://shangxueweilong.oss-cn-guangzhou.aliyuncs.com/20230713145211.png)

+ ***激活函数***不仅可以使得多层神经网络能拟合非线性问题还可以使得神经网络层数增加（如果没有激活函数，W1xW2xW3可以看作W0，原本多层网络实质上却是单层）理论上两层神经网络可以无限逼近任意近似连续函数,神经网络越多越能拟合函数但过多会造成过拟合的现象

---

+ ***两种激活函数的优缺点***

​      1.Sigmoid：饱和状态的点梯度近似为0，在链式求导中也为0使得后面的参数得不到更新

![](https://shangxueweilong.oss-cn-guangzhou.aliyuncs.com/20230713085229.png)

---

​        2.ReLu

![](https://shangxueweilong.oss-cn-guangzhou.aliyuncs.com/20230713085258.png)

+ ***输出层的激活函数及损失函数***

![](https://shangxueweilong.oss-cn-guangzhou.aliyuncs.com/20230713090155.png)

---

![](https://shangxueweilong.oss-cn-guangzhou.aliyuncs.com/20230713090354.png)

---

+ ## with torch_no_grad():

  在深度学习中，我们通过反向传播来计算模型参数的梯度。但是，在进行测试集验证或者模型推断时，并不需要计算梯度，这时计算梯度只会浪费计算资源。因此，为了提高运行效率，可以使用 torch.no_grad() 上下文管理器来指定这段代码块内部不需要计算梯度。

  在使用 torch.no_grad() 上下文管理器时，该代码块内的所有操作都将被视为不需要梯度计算，包括所有的前向计算，反向传播和参数更新等操作。这样可以节省显存和计算资源，并且减少计算时间。

+ ### 解决局部极小的方法

1. 以多组不同参数初始化神经网络相当于多个起点
2. 模拟退货：有一定概率接受次优解，从而有助于跳出次优解，随着时间推移这个概率要逐渐减小确保算法稳定
3. 随机梯度下降：在计算梯度时加入了随机因素，即使位于局部极小点他的梯度仍可能不为0，存在跳出的可能性



# 卷积神经网络

+ ***卷积核***：每个卷积核的参数固定，可以看作每个卷积核对应一个特征。打个比方，卷积核就像一双眼睛，人类视角有限，一眼望去，只能看到这世界的局部。如果一眼就看到全世界，你会累死，而且一下子接受全世界所有信息，你大脑接收不过来。当然，即便是看局部，针对局部里的信息人类双眼也是有偏重、偏好的。比如看人时，对脸、身材、头发等是重点关注。

+ ***卷积的in_channels和out_chaneels***：

  - in_channels：图像数据有多少层（几个特征）in_channels就等于几，例如RGB图像具有三层(因为其是由三原色组成)那么他的in_channels就应该为3
  - out_chaneels：输出结果的层数与特征数相同

+ ***权值共享* **：

  其实权值共享这个词说全了就是整张图片在使用同一个卷积核内的参数，比如一个331的卷积核，这个卷积核内9个的参数被整张图共享，而不会因为图像内位置的不同而改变卷积核内的权系数。说的再直白一些，就是用一个卷积核不改变其内权系数的情况下卷积处理整张图片（当然CNN中每一层不会只有一个卷积核的，这样说只是为了方便解释而已）。权值共享可以大大减小参数量

+ ***卷积层操作***：

![](https://shangxueweilong.oss-cn-guangzhou.aliyuncs.com/20230713085438.png)

+ ***最大池化***

![](https://shangxueweilong.oss-cn-guangzhou.aliyuncs.com/20230713152725.png)

+ ***池化与卷积***：卷积是为了提取图像特征，通过卷积层，可以自动提取图像的高维度且有效的特征，池化主要是为了降维，减少卷积层提取的特征个数，简而言之，池化就是去除杂余信息，保留关键信息




